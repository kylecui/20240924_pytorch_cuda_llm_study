{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda activate py_gpu\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7011, 0.5376, 0.9051, 0.7693],\n",
      "        [0.6151, 0.5230, 0.9866, 0.0510],\n",
      "        [0.9894, 0.8698, 0.7650, 0.3095],\n",
      "        [0.2850, 0.5232, 0.0903, 0.1520]])\n"
     ]
    }
   ],
   "source": [
    "# 测试torch工作正常，随机生成一个4x4的矩阵\n",
    "random_torch = torch.rand(4, 4)\n",
    "print(random_torch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "# 将输入的词汇表索引转换为指定维度的Embedding向量\n",
    "\n",
    "\n",
    "class TokenEmbedding(nn.Embedding):\n",
    "    # def __init__(self, num_embeddings: int, embedding_dim: int = 768):\n",
    "    #     super().__init__(num_embeddings, embedding_dim, padding_idx=0)\n",
    "    def __init__(self, vocab_size, d_model): # d_model是模型embedding的维度\n",
    "        # 用索引为1的token作为padding token\n",
    "        super(TokenEmbedding, self).__init__(vocab_size, d_model, padding_idx=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Embedding):\n",
    "    def __init__(self, d_model, max_len, device):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        # 初始化一个形状为(max_len, d_model)的全零矩阵\n",
    "        self.encoding = torch.zeros(max_len, d_model, device=device)\n",
    "        # 位置编码不需要参与反向传播\n",
    "        self.encoding.requires_grad = False\n",
    "\n",
    "        pos = torch.arange(0, max_len, device=device).float()\n",
    "        pos = pos.float().unsqueeze(dim=1) # 转换为浮点型 二维张量\n",
    "\n",
    "        _2i = torch.arange(0, d_model, step=2, device=device).float()\n",
    "\n",
    "        self.encoding[:, 0::2] = torch.sin(pos / (10000 ** (_2i / d_model)))\n",
    "        self.encoding[:, 1::2] = torch.cos(pos / (10000 ** (_2i / d_model)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len = x.size()\n",
    "        return self.encoding[:seq_len, :] # 返回的是位置编码矩阵的前seq_len行\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, max_len, drop_prob, device):\n",
    "        super(TransformerEmbedding, self).__init__()\n",
    "        self.tok_emb = TokenEmbedding(vocab_size, d_model)\n",
    "        self.pos_emb = PositionalEmbedding(d_model, max_len, device)\n",
    "        self.drop_out = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        tok_emb = self.tok_emb(x)\n",
    "        pos_emb = self.pos_emb(x)\n",
    "        return self.drop_out(tok_emb + pos_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-head Attention层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 512\n",
    "n_head = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_head):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.n_head = n_head\n",
    "        self.d_model = d_model\n",
    "        # 定义Q K V\n",
    "        self.W_q = nn.Linear(d_model, n_head) # Q\n",
    "        self.W_k = nn.Linear(d_model, n_head)\n",
    "        self.W_v = nn.Linear(d_model, n_head)\n",
    "        # 输出层\n",
    "        self.W_o = nn.Linear(n_head, d_model)\n",
    "        # softmax层\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        # q, k, v的形状是(batch_size, seq_len, d_model), 即批次大小，序列长度（时间），模型维度（词向量维度）\n",
    "        batch, time, dimension = q.shape\n",
    "\n",
    "        n_d = self.d_model // self.n_head # 每个头的维度\n",
    "        q, k, v = self.W_q(q), self.W_k(k), self.W_v(v) \n",
    "        # 将Q K V映射到多头\n",
    "        q_s = q.view(batch, time, self.n_head, n_d).permute(0, 2, 1, 3) # (batch, n_head, time, n_d)\n",
    "        k_s = k.view(batch, time, self.n_head, n_d).permute(0, 2, 1, 3)\n",
    "        v_s = v.view(batch, time, self.n_head, n_d).permute(0, 2, 1, 3)\n",
    "\n",
    "        # 计算Q K 点积， 即注意力分数\n",
    "        score = q_s @ k_s.transpose(2, 3) / math.sqrt(n_d) # (batch, n_head, time, time)\n",
    "\n",
    "        if mask is not None:\n",
    "            score = score.masked_fill(mask == 0, -1e9) # 掩码为0的位置填充为负无穷，即屏蔽时间序列以后的信息\n",
    "\n",
    "        # 计算注意力权重\n",
    "        score = self.softmax(score)@v_s \n",
    "        score = score.permute(0, 2, 1, 3).contiguous().view(batch, time, dimension) \n",
    "        out = self.W_o(score)\n",
    "        return out\n",
    "\n",
    "attention = MultiHeadAttention(d_model, n_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out = attention(x, x, x)\n",
    "# print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Normalization层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, d_model, eps=1e-12):  # eps 数值稳定性，非常小的常数\n",
    "        super(LayerNorm, self).__init__()\n",
    "        # y = gamma * x + beta\n",
    "        self.gamma = nn.Parameter(torch.ones(d_model)) # 缩放参数 初始化为1\n",
    "        self.beta = nn.Parameter(torch.zeros(d_model)) # 偏移参数 初始化为0\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True) # 计算均值\n",
    "        var = x.var(-1, unbiased=False, keepdim=True) # 计算方差 (unbiased=False表示不使用无偏估计，keepdim=True表示保持维度)\n",
    "        out = (x - mean) / torch.sqrt(var + self.eps) # 归一化\n",
    "        out = self.gamma * out + self.beta\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义 position-wised 前馈神经网络 类\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, hidden, dropout=0.1): # hidden是隐藏层的维度， dropout率，默认0.1防止过拟合\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        # 定义第一个全连接层\n",
    "        self.fc1 = nn.Linear(d_model, hidden) # 输入维度是d_model，输出维度是hidden\n",
    "        # 定义第二个全连接层\n",
    "        self.fc2 = nn.Linear(hidden, d_model) # 输入维度是hidden，输出维度是d_model\n",
    "        self.dropout = nn.Dropout(dropout) # 定义dropout率\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x) # 第一个全连接层\n",
    "        x = F.relu(x) # 激活函数\n",
    "        x = self.dropout(x) # dropout防止过拟合\n",
    "        x = self.fc2(x) # 第二个全连接层，回到d_model维度\n",
    "        return x\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个Transformer的Encoder Layer类\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_head, hidden, dropout=0.1): # d_model是模型embedding的维度，n_head是多头注意力的头数，hidden是前馈神经网络的隐藏层维度\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.attention = MultiHeadAttention(d_model, n_head) # 定义多头注意力层\n",
    "        self.norm1 = LayerNorm(d_model) # 定义LayerNorm 归一化层\n",
    "        self.dropout1 = nn.Dropout(dropout) # 定义第一个dropout层\n",
    "        self.ffn = PositionwiseFeedForward(d_model, hidden, dropout) # 定义前馈神经网络层\n",
    "        self.norm2 = LayerNorm(d_model) # 定义第二个LayerNorm 归一化层\n",
    "        self.dropout2 = nn.Dropout(dropout) # 定义第二个dropout层\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # 保存原始输入，用于残差连接\n",
    "        _x = x\n",
    "\n",
    "        x = self.attention(x, x, x, mask) # 多头注意力， QKV的输入都是x 这里的参数是定义模型的类里forward函数实现时定义的参数\n",
    "        x = self.dropout1(x) # dropout防止过拟合\n",
    "        x = self.norm1(x + _x) # 残差连接，归一化\n",
    "\n",
    "        # 保存这一阶段，用于下一个子层的残差连接\n",
    "        _x = x\n",
    "        x = self.ffn(x) # 前馈神经网络\n",
    "        x = self.dropout2(x) # dropout防止过拟合\n",
    "        x = self.norm2(x + _x) # 残差连接，归一化\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module): # 多层的Encoder Layer\n",
    "    # def __init__(self, d_model, n_head, hidden, n_layers, vocab_size, max_len, drop_prob, device):\n",
    "    def __init__(self, enc_voc_size, max_len, d_model, ffn_hidden, n_head, n_layers, dropout, device):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = TransformerEmbedding(enc_voc_size, d_model, max_len, dropout, device) # Transformer的Embedding层\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                EncoderLayer(d_model, ffn_hidden, n_head, device) for _ in range(n_layers)\n",
    "            ]\n",
    "        ) # 多层的Encoder Layer \n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.embedding(x) # Embedding层\n",
    "        for layer in self.layers: # 多层的Encoder Layer\n",
    "            x = layer(x, mask)\n",
    "        return x # 返回所有编码器返回的x （将被用作解码器decoder的输入的k-v）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, ffn_hidden, n_head, drop_prob):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.attention1 = MultiHeadAttention(d_model, n_head) # 第一个多头注意力层， 自注意力层\n",
    "        self.norm1 = LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(drop_prob)\n",
    "        self.cross_attention = MultiHeadAttention(d_model, n_head) # 第二个多头注意力层， 交叉注意力层, 用于encoder-decoder的attention, k-v来自encoder的输出\n",
    "        self.norm2 = LayerNorm(d_model)\n",
    "        self.dropout2 = nn.Dropout(drop_prob)\n",
    "        self.ffn = PositionwiseFeedForward(d_model, ffn_hidden) # 前馈神经网络， 用于解码器的输出\n",
    "        self.norm3 = LayerNorm(d_model)\n",
    "        self.dropout3 = nn.Dropout(drop_prob)\n",
    "\n",
    "    def forward(self, dec, enc, self_mask=None, enc_mask=None):  # dec是decoder的输入， enc是encoder的输出(用于第二层注意力的输入)，self_mask是自注意力层掩码， enc_mask是交叉注意力层掩码\n",
    "        _x = dec # 保存原始输入，用于残差连接\n",
    "        x = self.attention1(dec, dec, dec, self_mask) # 自注意力层， QKV的输入都是decoder的输入dec，这里的参数是定义模型的类里forward函数实现时定义的参数\n",
    "        x = self.dropout1(x)\n",
    "        x = self.norm1(x + _x) # 残差连接，归一化\n",
    "\n",
    "        _x = x # 保存这一阶段，用于下一个子层的残差连接\n",
    "\n",
    "        x = self.cross_attention(x, enc, enc, enc_mask) # 交叉注意力层， k-v来自encoder的输出enc， q来自上一层的输出x\n",
    "        x = self.dropout2(x)\n",
    "        x = self.norm2(x + _x) # 残差连接，归一化\n",
    "\n",
    "        _x = x # 保存这一阶段，用于下一个子层的残差连接 \n",
    "\n",
    "        x = self.ffn(x) # 前馈神经网络\n",
    "        x = self.dropout3(x)\n",
    "        x = self.norm3(x + _x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, dec_voc_size, max_len, d_model, ffn_hidden, n_head, n_layers, drop_prob, device):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = TransformerEmbedding(dec_voc_size, d_model, max_len, drop_prob, device) # 输入的词汇表，转为embedding向量\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                DecoderLayer(d_model, ffn_hidden, n_head, drop_prob) for _ in range(n_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.fc=nn.Linear(d_model, dec_voc_size) # 输出词汇表，将decoder的输出映射到词汇表的维度\n",
    "\n",
    "    def forward(self, dec, enc, self_mask=None, enc_mask=None): # 这里写的和视频不一样，应该是视频写错了\n",
    "        x = self.embedding(dec) # Embedding层，将decoder的输入部分dec转为embedding向量\n",
    "        for layer in self.layers: # 进入多层的Decoder Layer循环，每一层都要考虑两部分输入，一部分时decoder的embedding，一部分是encoder的输出，并且两部分都有掩码，分别是第一层的自注意力层掩码和第二层的交叉注意力层掩码\n",
    "            x = layer(x, enc, self_mask, enc_mask)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, \n",
    "                src_pad_idx,\n",
    "                trg_pad_idx,\n",
    "                enc_voc_size,\n",
    "                dec_voc_size,\n",
    "                d_model,\n",
    "                max_len,\n",
    "                n_heads,\n",
    "                ffn_hidden,\n",
    "                n_layers,\n",
    "                drop_prob,\n",
    "                device):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = Encoder(enc_voc_size, d_model, max_len, ffn_hidden, n_heads, n_layers, drop_prob, device)\n",
    "        self.decoder = Decoder(dec_voc_size, d_model, max_len, ffn_hidden, n_heads, n_layers, drop_prob, device)\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "\n",
    "    def make_pad_mask(self, q, k, pad_idx_q, pad_idx_k):\n",
    "        len_q, len_k = q.size(1), k.size(1)\n",
    "        q = q.ne(pad_idx_q).unsqueeze(1).unsqueeze(3)\n",
    "        q = q.repeat(1, 1, 1, len_k)\n",
    "        k = k.ne(pad_idx_k).unsqueeze(1).unsqueeze(2)\n",
    "        k = k.repeat(1, 1, len_q, 1)\n",
    "        mask = q & k\n",
    "        return mask\n",
    "    \n",
    "    def make_casual_mask(self, q, k):\n",
    "        len_q, len_k = q.size(1), k.size(1)\n",
    "        mask = torch.trill(torch.ones(len_q, len_k)).type(torch.BoolTensor).to(self.device)\n",
    "        return mask\n",
    "    \n",
    "    def forward(self, src, trg):\n",
    "        src_mask = self.make_pad_mask(src, src, self.src_pad_idx, self.src_pad_idx)\n",
    "        trg_mask = self.make_pad_mask(trg, trg, self.trg_pad_idx, self.trg_pad_idx) * self.make_casual_mask(trg, trg)\n",
    "        enc_src = self.encoder(src, src_mask) # encoder的输入是src，src_mask是自注意力层的掩码\n",
    "        out = self.decoder(trg, enc_src, trg_mask, src_mask) # decoder的输入是trg，enc_src是encoder的输出，trg_mask是自注意力层的掩码，src_mask是交叉注意力层的掩码\n",
    "        return out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
